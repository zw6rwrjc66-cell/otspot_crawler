import requests
from bs4 import BeautifulSoup
import datetime
from sqlalchemy.orm import Session
from database import Hotspot, SessionLocal
import json
import re
import time
import os
from playwright.sync_api import sync_playwright

def fetch_hotspot_details(url: str, source: str):
    """
    Fetch details for a specific hotspot URL using Playwright.
    Returns a tuple (content, screenshot_path, summary).
    """
    try:
        with sync_playwright() as p:
            # Launch browser in headless mode
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            # Navigate to the URL
            page.goto(url, wait_until="networkidle", timeout=30000)
            
            # Wait a bit for dynamic content to load
            page.wait_for_timeout(2000)
            
            # Get title BEFORE taking screenshot (while page is still active)
            try:
                title = page.title()
            except:
                title = "Unknown Title"
            
            # Take screenshot
            screenshot_dir = os.path.join(os.path.dirname(__file__), "static", "screenshots")
            os.makedirs(screenshot_dir, exist_ok=True)
            
            # Generate unique filename based on timestamp
            timestamp = int(time.time() * 1000)
            screenshot_filename = f"screenshot_{timestamp}.png"
            screenshot_path = os.path.join(screenshot_dir, screenshot_filename)
            
            page.screenshot(path=screenshot_path, full_page=True)
            
            # Extract text content
            # Try to get main content area (this is generic, can be customized per platform)
            content = page.inner_text('body')
            
            # Clean up excessive whitespace
            content = '\n'.join([line.strip() for line in content.split('\n') if line.strip()])
            
            # Limit content length
            if len(content) > 5000:
                content = content[:5000] + "\n\n... (content truncated)"
            
            # Close browser AFTER getting all data
            browser.close()
            
            # Generate a simple summary (in production, use an LLM API)
            summary = f"ğŸ“¸ Screenshot captured. Article: {title[:100]}..."
            
            # Return relative path for database storage
            relative_screenshot_path = f"/static/screenshots/{screenshot_filename}"
            
            return content, relative_screenshot_path, summary

    except Exception as e:
        return f"Error fetching details with Playwright: {str(e)}", None, "Failed to generate summary."

def fetch_weibo_hot():
    """
    Fetch Weibo Hot Search (å¾®åšçƒ­æœ)
    Uses Weibo's public API endpoint
    """
    url = "https://weibo.com/ajax/side/hotSearch"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://weibo.com"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            data = response.json()
            hot_list = []
            
            if 'data' in data and 'realtime' in data['data']:
                for idx, item in enumerate(data['data']['realtime'][:3]):
                    hot_list.append({
                        "title": item.get('note', item.get('word', '')),
                        "url": f"https://s.weibo.com/weibo?q=%23{item.get('word', '')}%23",
                        "hot_value": str(item.get('num', 0)),
                        "rank": idx + 1
                    })
            
            print(f"âœ… Weibo: Fetched {len(hot_list)} items")
            return hot_list
        else:
            print(f"âŒ Weibo failed: {response.status_code}")
            return []
    except Exception as e:
        print(f"âŒ Weibo error: {e}")
        return []

def fetch_zhihu_hot():
    """
    Fetch Zhihu Hot List (çŸ¥ä¹çƒ­æ¦œ)
    Uses Zhihu's public API
    """
    url = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total?limit=50"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.zhihu.com/hot"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            data = response.json()
            hot_list = []
            
            if 'data' in data:
                for item in data['data'][:3]:
                    target = item.get('target', {})
                    hot_list.append({
                        "title": target.get('title', ''),
                        "url": target.get('url', '').replace('api/v4/questions', 'question'),
                        "hot_value": target.get('detail_text', ''),
                        "rank": 0  # Will be set later
                    })
            
            print(f"âœ… Zhihu: Fetched {len(hot_list)} items")
            return hot_list
        else:
            print(f"âŒ Zhihu failed: {response.status_code}")
            return []
    except Exception as e:
        print(f"âŒ Zhihu error: {e}")
        return []

def fetch_baidu_hot():
    """
    Fetch Baidu Hot Search (ç™¾åº¦çƒ­æœ)
    """
    url = "https://top.baidu.com/api/board?platform=wise&tab=realtime"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://top.baidu.com/board?tab=realtime"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            data = response.json()
            hot_list = []
            
            if 'data' in data and 'cards' in data['data']:
                for card in data['data']['cards']:
                    if 'content' in card:
                        for item in card['content'][:3]:
                            hot_list.append({
                                "title": item.get('word', ''),
                                "url": item.get('url', ''),
                                "hot_value": item.get('hotScore', ''),
                                "rank": 0
                            })
                        break
            
            print(f"âœ… Baidu: Fetched {len(hot_list)} items")
            return hot_list
        else:
            print(f"âŒ Baidu failed: {response.status_code}")
            return []
    except Exception as e:
        print(f"âŒ Baidu error: {e}")
        return []

def fetch_douyin_hot():
    """
    Fetch Douyin Hot Search (æŠ–éŸ³çƒ­æœ)
    """
    url = "https://www.iesdouyin.com/web/api/v2/hotsearch/billboard/word/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.douyin.com/"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            data = response.json()
            hot_list = []
            
            if 'word_list' in data:
                for item in data['word_list'][:3]:
                    hot_list.append({
                        "title": item.get('word', ''),
                        "url": f"https://www.douyin.com/search/{item.get('word', '')}",
                        "hot_value": str(item.get('hot_value', 0)),
                        "rank": 0
                    })
            
            print(f"âœ… Douyin: Fetched {len(hot_list)} items")
            return hot_list
        else:
            print(f"âŒ Douyin failed: {response.status_code}")
            return []
    except Exception as e:
        print(f"âŒ Douyin error: {e}")
        return []

def fetch_toutiao_hot():
    """
    Fetch Toutiao Hot News (ä»Šæ—¥å¤´æ¡çƒ­æ¦œ)
    Using alternative public API
    """
    url = "https://www.toutiao.com/hot-event/hot-board/?origin=toutiao_pc"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.toutiao.com/"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            data = response.json()
            hot_list = []
            
            if 'data' in data:
                for item in data['data'][:3]:
                    hot_list.append({
                        "title": item.get('Title', item.get('title', '')),
                        "url": item.get('Url', item.get('url', '')),
                        "hot_value": str(item.get('HotValue', item.get('hot_value', 0))),
                        "rank": 0
                    })
            
            print(f"âœ… Toutiao: Fetched {len(hot_list)} items")
            return hot_list
        else:
            print(f"âŒ Toutiao failed: {response.status_code}")
            return []
    except Exception as e:
        print(f"âŒ Toutiao error: {e}")
        return []

def run_crawler_task():
    """
    Main crawler task that fetches from all sources
    """
    print("=" * 50)
    print("ğŸš€ Starting crawler task...")
    print("=" * 50)
    
    db = SessionLocal()
    total_count = 0
    
    try:
        # Define all crawlers
        crawlers = [
            ("å¾®åšçƒ­æœ", fetch_weibo_hot),
            ("çŸ¥ä¹çƒ­æ¦œ", fetch_zhihu_hot),
            ("ç™¾åº¦çƒ­æœ", fetch_baidu_hot),
            ("æŠ–éŸ³çƒ­æœ", fetch_douyin_hot),
            ("ä»Šæ—¥å¤´æ¡", fetch_toutiao_hot),
        ]
        
        for source_name, fetch_func in crawlers:
            try:
                print(f"\nğŸ“¡ Fetching {source_name}...")
                data = fetch_func()
                
                if data:
                    for idx, item in enumerate(data):
                        if item.get('title'):  # Only add if title exists
                            db.add(Hotspot(
                                source=source_name,
                                title=item['title'],
                                url=item.get('url', ''),
                                rank=idx + 1,
                                hot_value=str(item.get('hot_value', '')),
                                created_at=datetime.datetime.utcnow()
                            ))
                            total_count += 1
                else:
                    print(f"âš ï¸  {source_name}: No data returned")
                    
            except Exception as e:
                print(f"âŒ {source_name} failed: {e}")
                continue
        
        db.commit()
        print("\n" + "=" * 50)
        print(f"âœ… Crawler task completed! Total: {total_count} items")
        print("=" * 50)
        
    except Exception as e:
        print(f"\nâŒ Crawler task failed: {e}")
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    run_crawler_task()
